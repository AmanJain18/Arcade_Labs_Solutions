# Create a Streaming Data Lake on Cloud Storage: Challenge Lab.

### Run in Cloud Shell...
### `TOPIC_ID` will be given in `Task 1.`
### `REGION` will be given in `Challenge scenario.`
```bash
export TOPIC_ID=
```
```bash
export REGION=
```
```bash
export MESSAGE="Hello Google!"
```
```bash
PROJECT_ID=$(gcloud config get-value project)
BUCKET_NAME="${PROJECT_ID}-bucket"
gcloud config set compute/region $REGION
gcloud services disable dataflow.googleapis.com
gcloud services enable dataflow.googleapis.com
gcloud services enable cloudscheduler.googleapis.com
gsutil mb gs://$BUCKET_NAME
gcloud pubsub topics create $TOPIC_ID
```
## If You get `REGION` as us-central1 or of uscentral($)-$[1,2,3,4] then run below command.
```bash
gcloud app create --region=us-central
```
## If Any other `REGION` then run this
```bash
gcloud app create --region=$REGION
```
```bash
gcloud scheduler jobs create pubsub publisher-job --schedule="* * * * *" \
    --topic=$TOPIC_ID --message-body="$MESSAGE"
gcloud scheduler jobs run publisher-job
git clone https://github.com/GoogleCloudPlatform/java-docs-samples.git
cd java-docs-samples/pubsub/streaming-analytics
mvn compile exec:java \
-Dexec.mainClass=com.examples.pubsub.streaming.PubSubToGcs \
-Dexec.cleanupDaemonThreads=false \
-Dexec.args=" \
    --project=$PROJECT_ID \
    --region=$REGION \
    --inputTopic=projects/$PROJECT_ID/topics/$TOPIC_ID \
    --output=gs://$BUCKET_NAME/samples/output \
    --runner=DataflowRunner \
    --windowSize=2"
```
