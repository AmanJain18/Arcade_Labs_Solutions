# Cloud Composer: Copying BigQuery Tables Across Different Locations


## Task 1 Perform manually 
- **Composer in the search box.**
- **Follow steps/lab instructions given in task 1.**
---
### Run in the Cloud Shell...
### ---Task 2---
```bash
export PROJECT_ID=$(gcloud config get-value project)
gsutil mb -p $PROJECT_ID -l us gs://$PROJECT_ID-us
gsutil mb -p $PROJECT_ID -l eu gs://$PROJECT_ID-eu
```
---
### ---Task 3---
```bash
bq mk --location=eu nyc_tlc_EU
sudo apt-get install -y virtualenv
python3 -m venv venv
source venv/bin/activate
```
---
### Copy the name of the bucket which start with us-east1-composer-... `Paste it after =` or 3rd bucket NAME
```bash
DAGS_BUCKET=
```
# Wait till the Composer get created from Task 1- It will take 10-15 minutes, then paste the below commandsin Cloud Shell.
```bash
gcloud composer environments run composer-advanced-lab \
--location us-east1 variables -- \
set table_list_file_path /home/airflow/gcs/dags/bq_copy_eu_to_us_sample.csv
gcloud composer environments run composer-advanced-lab \
--location us-east1 variables -- \
set gcs_source_bucket $PROJECT_ID-us
gcloud composer environments run composer-advanced-lab \
--location us-east1 variables -- \
set gcs_dest_bucket $PROJECT_ID-eu

gcloud composer environments run composer-advanced-lab \
    --location us-east1 variables -- \
    get gcs_source_bucket
```

```bash
cd ~
gsutil -m cp -r gs://spls/gsp283/python-docs-samples .

gsutil cp -r python-docs-samples/third_party/apache-airflow/plugins/* gs://$DAGS_BUCKET/plugins

gsutil cp python-docs-samples/composer/workflows/bq_copy_across_locations.py gs://$DAGS_BUCKET/dags
gsutil cp python-docs-samples/composer/workflows/bq_copy_eu_to_us_sample.csv gs://$DAGS_BUCKET/dags
```